{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientDescentOptimization",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy0fDdkEKgYUcJs4Ka9hwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ownit4137/TIL/blob/main/DL%20from%20Scratch/1/GradientDescentOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vktXJJDPWTrg"
      },
      "source": [
        "# 경사 하강법 최적화\r\n",
        "\r\n",
        "## SGD(확률적)\r\n",
        "\r\n",
        "$W  \\leftarrow  W -  \\eta \\cfrac{\\partial L}{\\partial W} $\r\n",
        "\r\n",
        "- 손실 함수의 기울기를 따라 weight을 갱신\r\n",
        "- 비등방성(anisotropy) 함수일때는 탐색 경로가 비효율적, 지그재그 탐색\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvZTEiyOVYeV"
      },
      "source": [
        "# pseudo\r\n",
        "class SGD:\r\n",
        "  ...\r\n",
        "  self.lr = lr\r\n",
        "  ...\r\n",
        "\r\n",
        "  def update(self, params, grads):\r\n",
        "    for key in params.keys():\r\n",
        "      params[key] -= self.lr * grads[key]\r\n",
        "  \r\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhWAujmOXiQL"
      },
      "source": [
        "## 모멘텀\r\n",
        "\r\n",
        "$ v  \\leftarrow   \\alpha v -  \\eta \\cfrac{\\partial L}{\\partial W} $\r\n",
        "\r\n",
        "$W  \\leftarrow   W + v $\r\n",
        "\r\n",
        "- 물리학의 개념을 도입\r\n",
        "- 경사 하강법에 관성을 더해 줌\r\n",
        "- 공이 그릇의 바닥을 구르는 듯한 움직임을 주어 지그재그 현상을 줄임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnz7rid1ZHei"
      },
      "source": [
        "# pseudo\r\n",
        "class Momentum:\r\n",
        "  ...\r\n",
        "  self.lr = lr\r\n",
        "  self.momentum = momentum\r\n",
        "  self.v = None\r\n",
        "  ...\r\n",
        "\r\n",
        "  def update(self, params, grads):\r\n",
        "    if self.v == None:\r\n",
        "      self.v = {}\r\n",
        "      # 각 층마다 v를 0으로 초기화\r\n",
        "      # v는 가중치의 shape과 같음\r\n",
        "      for key, val in params.items():\r\n",
        "        self.v[key] = np.zeros_like(val)\r\n",
        "\r\n",
        "    for key in params.keys():\r\n",
        "      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\r\n",
        "      params[key] += self.v[key]\r\n",
        "    \r\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiJnGmU5Z_d9"
      },
      "source": [
        "## AdaGrad(Adaptive Gradient, 적응적)\r\n",
        "\r\n",
        "$h  \\leftarrow   h +  \\cfrac{\\partial L}{\\partial W}  \\odot  \\cfrac{\\partial L}{\\partial W} $\r\n",
        "\r\n",
        "$W  \\leftarrow   W - \\eta  \\cfrac{1}{ \\sqrt{h + \\epsilon}} \\cfrac{\\partial L}{\\partial W} $\r\n",
        "\r\n",
        "- 학습이 진행될수록 $h$가 커져 학습률이 줄어듬\r\n",
        "- 이전 변화가 크면 -> 학습률을 과감하게 줄임\r\n",
        "- 이전 변화가 작으면 -> 학습률을 미세하게 줄임\r\n",
        "- weight마다 학습률 감소가 각각 다르게 적용됨\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q0pO2-EmIZ1"
      },
      "source": [
        "class AdaGrad:\r\n",
        "  ...\r\n",
        "  self.lr = lr\r\n",
        "  self.h = None\r\n",
        "  ...\r\n",
        "  \r\n",
        "  def update(self, params, grads):\r\n",
        "    if self.h == None:\r\n",
        "      self.h = {}\r\n",
        "      # 각 층마다 h를 0으로 초기화\r\n",
        "      # h는 가중치의 shape과 같음\r\n",
        "      for key, val in params.items():\r\n",
        "        self.h[key] = np.zeros_like(val)\r\n",
        "\r\n",
        "    for key in params.keys():\r\n",
        "      self.h[key] += grads[key] * grads[key]\r\n",
        "      # 0으로 나누는 것을 방지할 작은 값 e\r\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\r\n",
        "    \r\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SMqPs_KqnIL"
      },
      "source": [
        "## Adam(Adaptive Moment Estimation)\r\n",
        "\r\n",
        "- Adadelta, RMSprop처럼 gradient 제곱의 지수적 평균을 사용하고, momentum처럼 gradient의 지수적 평균도 사용함\r\n",
        "\r\n",
        "\r\n",
        "> In addition to storing an exponentially decaying average of past squared gradients\r\n",
        " like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum"
      ]
    }
  ]
}