{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGD_2layer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPj3qfTLtzR3lUnWsH30boQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ownit4137/TIL/blob/main/DL%20from%20Scratch/1/SGD_2layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIT8NL_56hsn"
      },
      "source": [
        "# 확률적 경사 하강법\r\n",
        "\r\n",
        "## 개념\r\n",
        "\r\n",
        "### 교차 엔트로피 오차, CEE\r\n",
        "\r\n",
        "$E = - \\sum_i {t}_i \\log({y}_i) $\r\n",
        "\r\n",
        "- 정답일 때의 출력이 클수록 오차가 작아짐, 작을수록 오차가 커짐\r\n",
        "\r\n",
        "### 배치 학습\r\n",
        "\r\n",
        "- 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있게 최적화됨\r\n",
        "- I/O를 통해 데이터를 읽는 횟수를 줄여 순수 계산 수행 비율을 높임\r\n",
        "\r\n",
        "### 미분\r\n",
        "\r\n",
        "- 해석적 미분 : 수식을 전개해 미분하는 것\r\n",
        "- 수치 미분 : 함수 f의 어떤 x를 중심으로 그 전후의 차분을 계산\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Ku7SvTQh-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fd8cc1-352d-4919-b789-15b125ded8d3"
      },
      "source": [
        "from google.colab import drive \r\n",
        "drive.mount('/content/gdrive/')\r\n",
        "\r\n",
        "%cd /content/gdrive/MyDrive/'Colab Notebooks'/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ra7vUQv7tJ2"
      },
      "source": [
        "from dataset.mnist import load_mnist    # 책 코드\r\n",
        "from PIL import Image\r\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7bIuTV68LNz"
      },
      "source": [
        "def cross_entropy_error(y, t):\r\n",
        "  # 1차원 배열일 때의 처리, (1, n)꼴로 변환\r\n",
        "  if y.ndim == 1:\r\n",
        "    t = t.reshape(1, t.size)\r\n",
        "    y = y.reshape(1, y.size)\r\n",
        "      \r\n",
        "  # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\r\n",
        "  if t.size == y.size:\r\n",
        "    t = t.argmax(axis=1)\r\n",
        "            \r\n",
        "  batch_size = y.shape[0]\r\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\r\n",
        "\r\n",
        "\r\n",
        "def softmax(x):\r\n",
        "  if x.ndim == 2:\r\n",
        "    x = x.T\r\n",
        "    x = x - np.max(x, axis=0)\r\n",
        "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\r\n",
        "    return y.T \r\n",
        "\r\n",
        "  x = x - np.max(x) # 오버플로 대책\r\n",
        "  return np.exp(x) / np.sum(np.exp(x))\r\n",
        "\r\n",
        "\r\n",
        "def numerical_gradient(f, x):\r\n",
        "  h = 1e-4 # 0.0001\r\n",
        "  grad = np.zeros_like(x)\r\n",
        "  \r\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\r\n",
        "  while not it.finished:\r\n",
        "    idx = it.multi_index\r\n",
        "    tmp_val = x[idx]\r\n",
        "    x[idx] = float(tmp_val) + h\r\n",
        "    fxh1 = f(x) # f(x+h)\r\n",
        "    \r\n",
        "    x[idx] = tmp_val - h \r\n",
        "    fxh2 = f(x) # f(x-h)\r\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\r\n",
        "    \r\n",
        "    x[idx] = tmp_val # 값 복원\r\n",
        "    it.iternext()   \r\n",
        "      \r\n",
        "  return grad\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "  return 1 / (1 + np.exp(-x)) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnDfTcsGSew0"
      },
      "source": [
        "class SGD_2layer:\r\n",
        "  def __init__(self, input_size, hidden_size, output_size):\r\n",
        "    self.init_div = 0.01\r\n",
        "    self.params = {}\r\n",
        "    self.params['w1'] = self.init_div * np.random.randn(input_size, hidden_size)\r\n",
        "    self.params['w2'] = self.init_div * np.random.randn(hidden_size, output_size)\r\n",
        "    self.params['b1'] = np.zeros(hidden_size)\r\n",
        "    self.params['b2'] = np.zeros(output_size)\r\n",
        "\r\n",
        "  def predict(self, x):\r\n",
        "    w1, w2 = self.params['w1'], self.params['w2']\r\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\r\n",
        "\r\n",
        "    a1 = np.dot(x, w1) + b1\r\n",
        "    z1 = sigmoid(a1)\r\n",
        "    a2 = np.dot(z1, w2)\r\n",
        "    y = softmax(a2)\r\n",
        "\r\n",
        "    return y\r\n",
        "\r\n",
        "  def loss(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    return cross_entropy_error(y, t)\r\n",
        "\r\n",
        "  def accuracy(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    y = np.argmax(y, axis=1)  # 열 방향 최대\r\n",
        "    t = np.argmax(t, axis=1)\r\n",
        "\r\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "  def getgrad(self, x, t):\r\n",
        "    loss_w = lambda w : self.loss(x, t)\r\n",
        "\r\n",
        "    grads = {}\r\n",
        "    grads['w1'] = numerical_gradient(loss_w, self.params['w1'])\r\n",
        "    grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\r\n",
        "    grads['w2'] = numerical_gradient(loss_w, self.params['w2'])\r\n",
        "    grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\r\n",
        "    return grads\r\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_jBte6NycDs",
        "outputId": "1250ba77-12b0-4200-e5c3-0cf24bc3cef6"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\r\n",
        "x_train = x_train[:1000][:]\r\n",
        "t_train = t_train[:1000][:]\r\n",
        "\r\n",
        "\r\n",
        "net = SGD_2layer(input_size=784, hidden_size=10, output_size=10)\r\n",
        "\r\n",
        "iter_total = 100\r\n",
        "train_size = x_train.shape[0]\r\n",
        "batch_size = 50\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "train_loss_list = []\r\n",
        "train_acc_list = []\r\n",
        "test_acc_list = []\r\n",
        "\r\n",
        "iter_count = max(train_size / batch_size, 1)\r\n",
        "\r\n",
        "for i in range(iter_total):\r\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\r\n",
        "  x_batch = x_train[batch_mask]\r\n",
        "  t_batch = t_train[batch_mask]\r\n",
        "\r\n",
        "  grad = net.getgrad(x_batch, t_batch)\r\n",
        "  for key in('w1', 'b1', 'w2', 'b2'):\r\n",
        "    net.params[key] -= learning_rate * grad[key]\r\n",
        "\r\n",
        "  loss = net.loss(x_batch, t_batch)\r\n",
        "  train_loss_list.append(loss)\r\n",
        "\r\n",
        "  if i % iter_count == 0:\r\n",
        "    train_acc = net.accuracy(x_train, t_train)\r\n",
        "    test_acc = net.accuracy(x_test, t_test)\r\n",
        "    train_acc_list.append(train_acc)\r\n",
        "    test_acc_list.append(test_acc)\r\n",
        "    print(\"iter-\", i, \" train acc : \", train_acc, \" test_acc : \", test_acc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter- 0  train acc :  0.094  test_acc :  0.0958\n",
            "iter- 20  train acc :  0.116  test_acc :  0.1135\n",
            "iter- 40  train acc :  0.117  test_acc :  0.1028\n",
            "iter- 60  train acc :  0.117  test_acc :  0.1028\n",
            "iter- 80  train acc :  0.117  test_acc :  0.1028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWSJyotgUL4k"
      },
      "source": [
        "속도 문제로 데이터, hidden layer, 배치, iter을 줄여서 학습 => 정확도 낮음"
      ]
    }
  ]
}